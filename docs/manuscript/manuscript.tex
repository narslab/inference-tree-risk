\documentclass[11pt,twoside]{article}
\usepackage{etex, soul}
\newcommand{\num}{6{} }

\raggedbottom

%geometry (sets margin) and other useful packages
\usepackage{geometry}
\geometry{top=1.2in, left=1in,right=1in,bottom=1in,headsep=6pt}
 \usepackage{graphicx,booktabs,calc}

%=== GRAPHICS PATH ===========
\graphicspath{{./images/}}
% Marginpar width
%Marginpar width
\newcommand{\pts}[1]{\marginpar{ \small\hspace{0pt} \textit{[#1]} } }
\setlength{\marginparwidth}{.5in}
%\reversemarginpar
%\setlength{\marginparsep}{.02in}

%% Fonts
% \usepackage{fourier}
% \usepackage[T1]{pbsi}

\usepackage{lmodern}
\usepackage[T1]{fontenc}
%\usepackage{minted}

\usepackage{rotating}
%% Cite Title
\usepackage[style=numeric,backend=biber,natbib,sorting=none,maxcitenames=2,maxbibnames=99,doi=false,isbn=false,url=false,eprint=false]{biblatex}
\addbibresource{../ai-trees-references.bib}

%%% Counters
\usepackage{chngcntr,mathtools}
\counterwithout{figure}{section}
\counterwithout{table}{section}

\numberwithin{equation}{section}

%% Captions
\usepackage{caption}
\captionsetup{
  labelsep=quad,
  justification=raggedright,
  labelfont=sc
}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm}
\usepackage{bm}
\usepackage[mathscr]{eucal}
\usepackage{colortbl}
\usepackage{color}

\DeclareRobustCommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}

\usepackage{epstopdf,subfigure,hyperref,enumerate,polynom,polynomial}
\usepackage{multirow,minitoc,fancybox,array,multicol}

\definecolor{slblue}{rgb}{0,.3,.62}
\hypersetup{
    colorlinks,%
    citecolor=blue,%
    filecolor=blue,%
    linkcolor=blue,
    urlcolor=slblue
}

%%%TIKZ
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{pgfgantt}
\pgfplotsset{compat=newest}

\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{shadows,automata}
\usetikzlibrary{patterns,fit}
%\usetikzlibrary{circuits.ee.IEC}
\usetikzlibrary{decorations.text}
% For Sagnac Picture
\usetikzlibrary{%
    decorations.pathreplacing,%
    decorations.pathmorphing%
}

%
%Redefining sections as problems
%
\makeatletter
\newenvironment{question}{\@startsection
	{section}
	{1}
	{-.2em}
	{-3.5ex plus -1ex minus -.2ex}
    	{1.3ex plus .2ex}
    	{\pagebreak[3]%forces pagebreak when space is small; use \eject for better results
	\large\bf\noindent{Question }
	}
	}
	%{\vspace{1ex}\begin{center} \rule{0.3\linewidth}{.3pt}\end{center}}
	%\begin{center}\large\bf \ldots\ldots\ldots\end{center}}
\makeatother

%
%Fancy-header package to modify header/page numbering
%
%\renewcommand{\chaptermark}[1]{ \markboth{#1}{} }
\renewcommand{\sectionmark}[1]{ \markright{#1}{} }

\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparsep} %these change header-rule width
%\addtolength{\headwidth}{\marginparwidth}
%\fancyheadoffset{30pt}
%\fancyfootoffset{30pt}
\fancyhead[LO,RE]{\small  \it \nouppercase{\leftmark}}
\fancyhead[RO,LE]{\small Page \thepage}
\fancyfoot[RO,LE]{\small }% PR \num S-2015}
\fancyfoot[LO,RE]{\small }%\scshape MODL}
\cfoot{}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\footrulewidth}{0pt}
%\setlength\voffset{-0.25in}
%\setlength\textheight{648pt}


\usepackage{paralist}


%%% FORMAT PYTHON CODE
\usepackage{listings}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{8} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{8}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

%\usepackage{listings}

% % Python style for highlighting
% \newcommand\pythonstyle{\lstset{
% language=Python,
% basicstyle=\footnotesize\ttm,
% otherkeywords={self},             % Add keywords here
% keywordstyle=\footnotesize\ttb\color{deepblue},
% emph={MyClass,__init__},          % Custom highlighting
% emphstyle=\footnotesize\ttb\color{deepred},    % Custom highlighting style
% stringstyle=\color{deepgreen},
% frame=tb,                         % Any extra options here
% showstringspaces=false            %
% }}

% % Python environment
% \lstnewenvironment{python}[1][]
% {
% \pythonstyle
% \lstset{#1}
% }
% {}

% % Python for external files
% \newcommand\pythonexternal[2][]{{
% \pythonstyle
% \lstinputlisting[#1]{#2}}}

% % Python for inline
% \newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


\newcommand{\osn}{\oldstylenums}
\newcommand{\dg}{^{\circ}}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\pt}{\phantom}
\newcommand{\tf}{\therefore}
\newcommand{\?}{\stackrel{?}{=}}
\newcommand{\fr}{\frac}
\newcommand{\dfr}{\dfrac}
%\newcommand{\ul}{\underline}
\newcommand{\tn}{\tabularnewline}
\newcommand{\nl}{\newline}
\newcommand\relph[1]{\mathrel{\phantom{#1}}}
\newcommand{\cm}{\checkmark}
\newcommand{\ol}{\overline}
\newcommand{\rd}{\color{red}}
\newcommand{\bl}{\color{blue}}
\newcommand{\pl}{\color{purple}}
\newcommand{\og}{\color{orange!90!black}}
\newcommand{\gr}{\color{green!40!black}}
\newcommand{\nin}{\noindent}
\newcommand{\la}{\lambda}
\renewcommand{\th}{\theta}
\newcommand{\al}{\alpha}
\newcommand{\G}{\Gamma}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,thick,inner sep=1pt] (char) {\small #1};}}

\newcommand{\bc}{\begin{compactenum}[\quad--]}
\newcommand{\ec}{\end{compactenum}}

\newcommand{\p}{\partial}
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\dpd}[2]{\dfrac{\partial{#1}}{\partial{#2}}}
\newcommand{\pdd}[2]{\frac{\partial^2{#1}}{\partial{#2}^2}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\vspace{-16ex}
\title{Predicting tree failure likelihood for utility risk mitigation via a novel convolutional neural network}
%\author{}
\date{}
\maketitle


\section{Introduction}
Utilities have been concerned with tree-related outages for many years, but much of the work to quantify them has been done in-house: Utilities contract with a consultant who analyzes tree-related outages (or interruptions). In general, tree-related outages are due to contact between tree parts and power lines. Sometimes, the contact is due to growth of branches into the lines; sometimes it is due to branch failure; sometimes it is due to whole-tree failure (either uprooting or trunk failure). When outages occur, the economic costs can be substantial. For example, from 2005 to 2015, \cite{graziano2020wider} computed the annual economic cost of outages in the US state of Connecticut; the mean was approximately \$8.3 bn.  Outages due to contact from branch growth or branch failure can be mitigated by line clearance pruning. Estimates vary, but the annual amount that utilities spend on tree trimming operations is typically in the billions of dollars \cite{guggenmoos2003effects}. Despite the comparatively high annual cost of pruning, studies have repeatedly shown that pruning branches away from the wires reduces the number of outages (sometimes classified as “preventable” outages). In Massachusetts, USA, for instance, tree failure was responsible 40\% of preventable tree-caused outages, but removing or severely pruning high risk trees improved reliability by 20\% to 30\% \cite{simpson1996treecaused}. A more recent (and sophisticated) analysis in Connecticut showed that enhanced tree pruning reduced outages during storms \cite{parent2019analysis}. And using a statistical model to predict outages based on data from two Gulf Coast states in the USA, \citet{nateghi2014power} demonstrated that prediction models were less uncertain when they included the effect of utility pruning; models including wind speed without considering the effect of trimming were not as accurate.

%Utility pruning can be effective because both structurally-deficient and structurally-sound branches adjacent to or overhanging
%the lines in proximity are removed or reduced, reducing the likelihood of contact.
Utility pruning aims to reduce the frequency of tree-caused outages through the reduction or removal of branches in proximity to overhead electrical lines.
% Pruning also reduces external loads on the tree (e.g., wind or snow and ice), which reduces the likelihood of
% whole-tree failure, whether by uprooting or stem breakage.  Although the annual cost of utility vegetation management is 
% large, the economic cost of outages can far exceed it. A recent benefit cost analysis demonstrated a highly favorable
% benefit cost ratio for pruning relative to the economic costs of outages (Graziano et al. 2020).  
However, it cannot completely eliminate tree-related outages. Failure of trees away from the right-of-way can still impact the lines and cause outages \cite{guggenmoos2003effects}. The proportion of tree failures away from the wires that cause outages varies and has not been rigorously quantified. \citet{guggenmoos2011treerelated} estimated that 95\% of tree-caused outages in the Pacific Northwest region of the US, were due to tree failure, and \citet{wismer2018targeted} reported approximately 25\% of interruptions in Illinois, USA, were caused by trees that uprooted or broke in the stem. Predicting the likelihood of failure is an inexact science, but tree risk assessment best management practices have been developed \cite{e.thomassmiley2017best,johnw.goodfellow2020best}. Risk includes assessing the likelihood of failure, the likelihood of impact, and the severity of consequences. The likelihood of failure depends on the anticipated loads on the tree and its load-bearing capacity. The likelihood of impact depends on proximity to the target (the lines, poles, and other hardware---``infrastructure''---in the case of utility tree risk assessment), the target’s occupancy rate (which is constant for utility lines) and whether the target is sheltered, for example by neighboring trees. Severity of consequences depends on the damage done to the infrastructure and, more importantly in some cases, the economic costs and disruption associated with outages---this, in turn, is partially related to the size of the tree or tree part that fails, and how much momentum it has when it impacts the infrastructure.

Individual tree risk assessment can be costly because of the time it requires. In some situations, a less time-consuming assessment may be justified to reduce costs, i.e.\ a ``Level 1'' assessment (Smiley et al. 2017). Studies have shown that trees with greater risk ratings were more likely to be detected from Level 1 risk assessments conducted in a moving vehicle in Rhode Island, USA \cite{rooney2005reliability} and Florida, USA \cite{koeser2016frequency} . The utility of Level 1 assessments in these areas suggests that artificial intelligence (AI) tools may be an effective way to reduce the cost of tree risk assessment.

AI-based image analysis is relatively widely used, even in engineering applications, such as earthquake risk assessment \cite{jiao2020artificial,salehi2018emerging} and structural health monitoring \cite{spencer2019advances}. A relevant application for tree species identification using a convolutional neural network (CNN) was even recently demonstrated \cite{fricker2019convolutional}. Yet, AI has not been applied to the problem of tree-utility line risk assessment---one that is complicated by the very large number of tree species to be considered, seasonal variation in tree appearance and associated risk and local meteorological conditions. However, the flexibility and power of CNNs, appears promising. In this paper, we demons In the remainder of this paper, we first provide a background on CNN which is critical for sustainability in critical infrastructural systems. Our goal is to further demonstrate an innovative automated approach to tree risk assessment using an AI tool that can be readily deployed for use in various locations and also continually improved through subsequent training on new datasets.


\section{Background}
%Neural networks developed over the past several decades from the introduction of the single-layer perceptron.
The groundbreaking study of \citet{hubel1959receptive} showed that visual perception in cats was a result of the
activation or inhibition of groups of cells in the visual cortex known as ``receptive fields.''  Further, they attempted
to map the cortical architecture in cats and monkeys \cite{hubel1962receptive,hubel1965receptive,hubel1968receptive}.
Subsequent attempts were then made to model neural networks that could be trained to automatically recognize visual patterns with modest performance \cite{rosenblatt1962principles,kabrisky1966proposed,giebel1971feature,fukushima1975cognitron}. However, the breakthrough came with ``neocognitron'' \cite{fukushima1980neocognitron}, which
was a self-learning neural network for pattern recognition that was robust to changes in position and shape distortion, a problem that plagued earlier efforts, including ``cognitron'' \cite{fukushima1975cognitron} proposed a few years earlier.

A few notable efforts demonstrated the neural networks for handwritten digit recognition
\cite{fukushima1988neocognitron,denker1988neural}, but these required significant preprocessing and feature
extraction. \cite{lecun1989handwritten} soon afterward introduced a multilayer neural network that mapped a feature in each neuron (representing a ``local receptive field'') via convolution. This network could also be trained by backpropagation like other existing neural networks and featured pooling operations for better distortion and
translation invariance. Further developments from this milestone yielded the LeNet-5 convolutional neural network which attained accuracy levels that rendered it commercially viable.

The big data revolution coupled with technological advancements that have made it possible to capture and store high resolution images have raised challenges that continue to be surmounted with successively high-performing
architectures. Over the past decade, some of these efforts resulted in significant breakthroughs in performance. AlexNet \cite{krizhevsky2012imageneta}, with 5 convolutional layers and 3 dense layers---one of the largest CNNs of its time, won the ILSVRC-2012\footnote{ImageNet Large Scale Visual Recognition Challenge; held annually from 2010 through 2017.} competition with a top-5 error rate of 15.3\% and served as a landmark in the Deep Learning subdomain. \citet{zeiler2014visualizing} then introduced ZFNet, besting the performance of AlexNet, and pioneered visualization techniques that were foundational for model inference and interpretability.  In the same year, GoogLeNet, a 22-layer network, was proposed \cite{szegedy2014going}, featuring the novel ``Inception module,'' which allowed for efficiency and accuracy in a very deep network. Subsequent improvements have been proposed to the original inception framework \cite{szegedy2015rethinking,szegedy2016inceptionv4}.  VGGNet \cite{simonyan2015very} also pushed the boundaries of depth with up 19 layers, achieving state-of-the-art performance at ILSVRC-2014. Finally, ResNet \cite{he2015deep} addressed the accuracy degradation problem that arises with increasing depth in a network by succesively fitting smaller sets of layers to the residual and employing skip connections. With these innovations, an unprecedented level of depth was achieved. Implementations with with 34, 50, 101 and 152 layers were demonstrated. ResNet-152 won first place in ILSVRC-2015.

Along with these developments in their architectures, CNNs have demonstrated viability for applications to image
classification, object and text detection, object and document tracking, labeling, speech, among several other related
fields \cite{gu2018recent}. \hl{Paragraph on applications to be completed---with particular attention paid to tree-related efforts.}


\section{Data and Methods}
\subsection{Image data description}
The training dataset consists of 505 images, each having an original size of $4032\times3024$ pixels. 
Based on the  ISA’s Tree Risk Assessment protocol, four categories of tree failure likelihood are defined. In this paper we only focus on three:
\begin{itemize}
\item \textbf{Probable}: failure expected under normal weather conditions within a given time frame
\item \textbf{Possible}: failure expected under extreme weather conditions; but unlikely during normal weather conditions
\item \textbf{Improbable}: failure unlikely either during normal or extreme weather conditions 
\end{itemize}

\subsection{Pre-processing}

\subsection{Convolutional neural network}


\begin{table}[h!]
  \centering
  \begin{tabular}{l l l l l l l }\toprule
    \bf Model & \multicolumn{3}{c}{\bf Training metrics} &\multicolumn{3}{c}{\bf Validation metrics}  \\\midrule
    & Error & Precision & Recall     & Error & Precision & Recall \\
    SafeTree & & & & & & \\
    GoogleNet (InceptionV3) & & & & & & \\
    ResNet50 & & & & & & \\
        VGGNet & & & & & & \\
    AlexNet & & & & & & \\\bottomrule
  \end{tabular}
  \caption{Comparing our model SafeTree with state-of-the-art CNN architectures trained on our data}
  \label{tab:comp}
\end{table}


\section{Results}

\subsection{Sensitivity to training resolution}

\section{Conclusion}

\printbibliography

\appendix


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
