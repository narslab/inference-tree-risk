{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as tkr\n",
    "\n",
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../python/\")\n",
    "from helpers import *\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "NUM_CHANNELS = 3\n",
    "IMAGE_WIDTH_LIST = [189]#, 252, 336]\n",
    "SCENARIO_LIST = [\"Pr_Po_Im\"] #, PrPo_Im \"Pr_Im\", \"Pr_PoIm\", \"Pr_Po_Im\"]\n",
    "NUM_EPOCHS = 20\n",
    "SAVED_MODEL_DIR = '../../results/models/'\n",
    "MODEL_PERFORMANCE_METRICS_DIR = '../../results/model-performance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image sets\n",
    "IMAGE_SETS_SQUARE_TRAIN = createResolutionScenarioImageDict(IMAGE_WIDTH_LIST, SCENARIO_LIST, train=True, rectangular = False)\n",
    "IMAGE_SETS_SQUARE_TEST = createResolutionScenarioImageDict(IMAGE_WIDTH_LIST, SCENARIO_LIST, train=False, rectangular = False)\n",
    "IMAGE_SETS_RECT_TRAIN = createResolutionScenarioImageDict(IMAGE_WIDTH_LIST, SCENARIO_LIST, train=True, rectangular = True)\n",
    "IMAGE_SETS_RECT_TEST = createResolutionScenarioImageDict(IMAGE_WIDTH_LIST, SCENARIO_LIST, train=False, rectangular = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def __init__(self, val_data):#, batch_size = 64):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        xVal, yVal = self.validation_data\n",
    "        val_pred = np.argmax(np.asarray(self.model.predict(xVal)), axis=1)\n",
    "        val_true = np.argmax(yVal, axis=1)        \n",
    "        _val_f1 = f1_score(val_true, val_pred, average='macro', zero_division = 0)\n",
    "        _val_precision = precision_score(val_true, val_pred, average='macro', zero_division = 0)\n",
    "        _val_recall = recall_score(val_true, val_pred, average='macro', zero_division = 0)\n",
    "\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        logs[\"val_f1\"] = _val_f1\n",
    "        logs[\"val_recall\"] = _val_recall\n",
    "        logs[\"val_precision\"] = _val_precision\n",
    "        print('— val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCNN(image_width, image_height, num_channels=3):\n",
    "    image_shape = (image_width, image_height, num_channels)\n",
    "    print('Image shape: ', image_shape)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = 5, strides = 2, activation=\"relu\", padding=\"same\", \n",
    "        input_shape = image_shape ))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(2))\n",
    "    model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(layers.MaxPooling2D(2))\n",
    "    model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(layers.MaxPooling2D(2))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(units = 408, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "    model.add(layers.Dropout(.3))\n",
    "\n",
    "    model.add(layers.Dense(units = 408, activation = 'relu'))\n",
    "    model.add(layers.Dropout(.3))\n",
    "\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = .001), #learning_rate = hp_learning_rate not defined\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelWithDetailedMetrics(image_width, scenario, num_epochs = 10, trial_seed = 1, rectangular = True, testing = True): \n",
    "    # IMAGES (former approach)\n",
    "    # training_images_and_labels, test_images_and_labels = splitData(image_sets[image_size][scenario], prop = 0.8, seed_num = trial_seed)\n",
    "    # training_images, training_labels = getImageAndLabelArrays(training_images_and_labels)\n",
    "    # test_images, test_labels = getImageAndLabelArrays(test_images_and_labels)\n",
    "    if rectangular:\n",
    "        image_height = getRectangularImageHeight(image_width)\n",
    "    else:\n",
    "        image_height = image_width\n",
    "\n",
    "    class_labels = getClassLabels(scenario)\n",
    "    print(\"Class labels:\", class_labels)\n",
    "\n",
    "    if rectangular==True:\n",
    "        image_dictionary_train = IMAGE_SETS_RECT_TRAIN\n",
    "        image_dictionary_test = IMAGE_SETS_RECT_TEST\n",
    "    else:\n",
    "        image_dictionary_train = IMAGE_SETS_SQUARE_TRAIN\n",
    "        image_dictionary_test = IMAGE_SETS_SQUARE_TEST\n",
    "\n",
    "        \n",
    "    training_images = np.array([x[0] for x in image_dictionary_train[image_width][scenario]]) ## TOD)\n",
    "    training_labels = np.array([x[1] for x in image_dictionary_train[image_width][scenario]]) \n",
    "    test_images = np.array([x[0] for x in image_dictionary_test[image_width][scenario]]) ## TOD)\n",
    "    test_labels = np.array([x[1] for x in image_dictionary_test[image_width][scenario]]) \n",
    "#     training_images = np.array([np.expand_dims(x[0],axis=2) for x in image_dictionary_train[image_width][scenario]]) ## TOD)\n",
    "#     training_labels = np.array([x[1] for x in image_dictionary_train[image_width][scenario]]) \n",
    "#     test_images = np.array([np.expand_dims(x[0],axis=2) for x in image_dictionary_test[image_width][scenario]]) ## TOD)\n",
    "#     test_labels = np.array([x[1] for x in image_dictionary_test[image_width][scenario]]) \n",
    "\n",
    "    print(\"Number of class training images:\", training_labels.sum(axis=0), \"total: \", training_labels.sum())\n",
    "    print(\"Number of class test images:\", test_labels.sum(axis=0), \"total: \", test_labels.sum())\n",
    "    \n",
    "    # CALLBACKS\n",
    "    model_metrics = Metrics(val_data=(test_images, test_labels))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # INIT MODEL AND PARAMS, FIT\n",
    "    K.clear_session()\n",
    "    #input_shape = (image_size, image_size, NUM_CHANNELS) ## shape of images\n",
    "    if testing:\n",
    "        model = testCNN(image_width, image_height, num_channels=NUM_CHANNELS)\n",
    "    else:\n",
    "        model = constructOptBaseCNN(image_width, image_height, scenario, num_channels = NUM_CHANNELS)    ## get model\n",
    "        opt_learning_rate = getOptCNNHyperparams(image_width, image_height, scenario)['learning_rate']    ## learning rate\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = opt_learning_rate)    \n",
    "    reset_weights(model) # re-initialize model weights\n",
    "    if testing:\n",
    "        model.compile(loss='categorical_crossentropy', metrics =  ['accuracy'])     ## compile and fit\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer = opt, metrics =  ['accuracy'])     ## compile and fit\n",
    "    hist = model.fit(training_images, training_labels, batch_size = 32, epochs = num_epochs, verbose=1, \n",
    "                     validation_data=(test_images, test_labels),\n",
    "                     callbacks = [model_metrics]) #, early_stopping])     \n",
    "    \n",
    "    # SAVE MODEL, SUMMARY AND PERFORMANCE\n",
    "    if testing == True:\n",
    "        model_name = \"test-opt-cnn-\" + scenario + \"-w-\" + str(image_width) + \"-px-h-\" + str(image_height) + \"-px\"\n",
    "    else:\n",
    "        model_name = \"opt-cnn-\" + scenario + \"-w-\" +str(image_width) + \"-px-h-\" + str(image_height) + \"-px\"\n",
    "    model_folder = \"model\"\n",
    "    if not os.path.exists(SAVED_MODEL_DIR):  \n",
    "        os.makedirs(SAVED_MODEL_DIR)\n",
    "    model.save(os.path.join(SAVED_MODEL_DIR, model_name, model_folder))     ## Save model summary\n",
    "    #print(os.path.join(SAVED_MODEL_DIR, model_name, \"summary.txt\"))\n",
    "    with open(os.path.join(SAVED_MODEL_DIR, model_name, \"summary.txt\"), 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    with open(os.path.join(SAVED_MODEL_DIR, model_name, \"history.txt\"), 'w') as f:\n",
    "        f.write(json.dumps(hist.history))    \n",
    "   \n",
    "    # ANALYZE PERFORMANCE AND SAVE OUTPUTS\n",
    "    y_pred = np.argmax(model.predict(test_images), axis=-1)     ## Params\n",
    "    ## Classification report\n",
    "    report = classification_report(np.argmax(test_labels, axis=-1), y_pred, zero_division=0,\n",
    "                                   labels = np.arange(len(class_labels)), target_names=class_labels, output_dict=True)\n",
    "    print(\"Classification report for scenario \" + scenario + \", width: \" + str(image_width) + \", height: \" + str(image_height) + \":\")\n",
    "    report = pd.DataFrame(report).transpose().round(2)\n",
    "    if not os.path.exists('../../results/classification-reports/'):  \n",
    "        os.makedirs('../../results/classification-reports/')\n",
    "    classification_report_suffix = scenario + \"-w-\" + str(image_width) + \"-h-\" + str(image_height) + \"px.csv\"\n",
    "    if testing == True:\n",
    "        report.to_csv(\"../../results/classification-reports/test-opt-classification-report-\" + classification_report_suffix)\n",
    "    else:\n",
    "        report.to_csv(\"../../results/classification-reports/opt-classification-report-\" + classification_report_suffix)        \n",
    "    print(report)\n",
    "    \n",
    "    ## Confusion matrix\n",
    "    con_mat = tf.math.confusion_matrix(labels=np.argmax(test_labels, axis=-1), predictions=y_pred).numpy()\n",
    "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    con_mat_df = pd.DataFrame(con_mat_norm, index = class_labels, columns = class_labels)\n",
    "    #print(\"Confusion matrix for scenario \" + scenario + \", resolution: \" + str(image_size) + \":\")\n",
    "    #print(con_mat_df)\n",
    "    figure = plt.figure()#figsize=(4, 4))    ## Confusion matrix heatmap\n",
    "    ax = sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues, fmt='g', cbar = False, annot_kws={\"size\": 16})\n",
    "    #figure.tight_layout()\n",
    "    plt.ylabel('True',fontsize=16)\n",
    "    ax.set_yticklabels(class_labels,va='center',fontsize=14)\n",
    "    ax.set_xticklabels(class_labels, ha='center',fontsize=14)\n",
    "    plt.xlabel('Predicted',fontsize=16)\n",
    "    #plt.show()\n",
    "    file_suffix =  scenario + \"-w-\" + str(image_width) + \"-px-h-\" + str(image_height) + \"-px.png\"\n",
    "    if testing == True:\n",
    "        con_mat_heatmap_file = \"../../figures/test-opt-confusion-matrix-\" +  file_suffix\n",
    "    else:\n",
    "        con_mat_heatmap_file = \"../../figures/opt-confusion-matrix-\" +  file_suffix\n",
    "    figure.savefig(con_mat_heatmap_file, dpi=180)#, bbox_inches='tight')\n",
    "    return(model, hist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScenarioModelPerformance(width = 189, num_epochs = 15, seed_val = 1, rect_boolean = True, test_boolean = True):\n",
    "    df = pd.DataFrame()\n",
    "    if rect_boolean:\n",
    "        height = getRectangularImageHeight(width)\n",
    "    else:\n",
    "        height = width\n",
    "    for s in SCENARIO_LIST:\n",
    "        m, h = trainModelWithDetailedMetrics(width, s, num_epochs, trial_seed = seed_val, \n",
    "            rectangular = rect_boolean, testing = test_boolean)\n",
    "        #visualizeCNN(m, s, width, images_per_class = 4, trial_seed = seed_val, testing = test_boolean)       \n",
    "        perf = pd.DataFrame.from_dict(h.history)\n",
    "        perf[['Scenario']] = s\n",
    "        perf['epoch'] = perf.index + 1\n",
    "        df = df.append(perf, ignore_index=True)\n",
    "        #del m\n",
    "    if test_boolean == True:\n",
    "        df_filename = \"../../results/test-opt-cnn-performance-metrics-summary-w-\" + str(width) + \"-px-h\" + str(height) + \"-px.csv\"\n",
    "    else:\n",
    "        df_filename = \"../../results/opt-cnn-performance-metrics-summary-w-\" + str(width) + \"-px-h\" + str(height) +  \"-px.csv\"\n",
    "    df.to_csv(df_filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['Probable', 'Possible', 'Improbable']\n",
      "Number of class training images: [740 208 116] total:  1064\n",
      "Number of class test images: [89 26 18] total:  133\n",
      "Image shape:  (189, 189, 3)\n",
      "Epoch 1/13\n",
      "34/34 [==============================] - 29s 802ms/step - loss: 1.1728 - accuracy: 0.5289 - val_loss: 1.8128 - val_accuracy: 0.6391\n",
      "— val_f1: 0.314476 — val_precision: 0.598118 — val_recall 0.342200\n",
      "Epoch 2/13\n",
      "34/34 [==============================] - 27s 797ms/step - loss: 0.9335 - accuracy: 0.6411 - val_loss: 2.0751 - val_accuracy: 0.6692\n",
      "— val_f1: 0.267267 — val_precision: 0.223058 — val_recall 0.333333\n",
      "Epoch 3/13\n",
      "34/34 [==============================] - 28s 832ms/step - loss: 0.8993 - accuracy: 0.6390 - val_loss: 2.8592 - val_accuracy: 0.6692\n",
      "— val_f1: 0.300000 — val_precision: 0.390585 — val_recall 0.348107\n",
      "Epoch 4/13\n",
      "34/34 [==============================] - 28s 819ms/step - loss: 0.8657 - accuracy: 0.6507 - val_loss: 1.9058 - val_accuracy: 0.6692\n",
      "— val_f1: 0.269697 — val_precision: 0.226463 — val_recall 0.333333\n",
      "Epoch 5/13\n",
      "34/34 [==============================] - 28s 810ms/step - loss: 0.8517 - accuracy: 0.6627 - val_loss: 1.2721 - val_accuracy: 0.3308\n",
      "— val_f1: 0.299151 — val_precision: 0.340894 — val_recall 0.352204\n",
      "Epoch 6/13\n",
      "34/34 [==============================] - 28s 810ms/step - loss: 0.7942 - accuracy: 0.6862 - val_loss: 2.5764 - val_accuracy: 0.6692\n",
      "— val_f1: 0.267267 — val_precision: 0.223058 — val_recall 0.333333\n",
      "Epoch 7/13\n",
      "34/34 [==============================] - 28s 837ms/step - loss: 0.8881 - accuracy: 0.6394 - val_loss: 1.3071 - val_accuracy: 0.6391\n",
      "— val_f1: 0.261137 — val_precision: 0.221354 — val_recall 0.318352\n",
      "Epoch 8/13\n",
      "34/34 [==============================] - 30s 868ms/step - loss: 0.7726 - accuracy: 0.6534 - val_loss: 16.7969 - val_accuracy: 0.1955\n",
      "— val_f1: 0.109015 — val_precision: 0.065163 — val_recall 0.333333\n",
      "Epoch 9/13\n",
      "34/34 [==============================] - 29s 857ms/step - loss: 0.7214 - accuracy: 0.6938 - val_loss: 1.2603 - val_accuracy: 0.6692\n",
      "— val_f1: 0.267267 — val_precision: 0.223058 — val_recall 0.333333\n",
      "Epoch 10/13\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.6828 - accuracy: 0.7111 - val_loss: 1.1209 - val_accuracy: 0.6466\n",
      "— val_f1: 0.264209 — val_precision: 0.223958 — val_recall 0.322097\n",
      "Epoch 11/13\n",
      "34/34 [==============================] - 29s 841ms/step - loss: 0.6141 - accuracy: 0.7324 - val_loss: 1.4082 - val_accuracy: 0.6617\n",
      "— val_f1: 0.265460 — val_precision: 0.222222 — val_recall 0.329588\n",
      "Epoch 12/13\n",
      "34/34 [==============================] - 28s 824ms/step - loss: 0.6042 - accuracy: 0.7440 - val_loss: 1.3282 - val_accuracy: 0.6617\n",
      "— val_f1: 0.266667 — val_precision: 0.223919 — val_recall 0.329588\n",
      "Epoch 13/13\n",
      "34/34 [==============================] - 28s 817ms/step - loss: 0.5502 - accuracy: 0.7779 - val_loss: 2.1263 - val_accuracy: 0.6692\n",
      "— val_f1: 0.267267 — val_precision: 0.223058 — val_recall 0.333333\n",
      "INFO:tensorflow:Assets written to: ../../results/models/test-opt-cnn-Pr_Po_Im-w-189-px-h-189-px\\model\\assets\n",
      "Classification report for scenario Pr_Po_Im, width: 189, height: 189:\n",
      "              precision  recall  f1-score  support\n",
      "Probable           0.67    1.00      0.80    89.00\n",
      "Possible           0.00    0.00      0.00    26.00\n",
      "Improbable         0.00    0.00      0.00    18.00\n",
      "accuracy           0.67    0.67      0.67     0.67\n",
      "macro avg          0.22    0.33      0.27   133.00\n",
      "weighted avg       0.45    0.67      0.54   133.00\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Scenario'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e55f73803ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgetScenarioModelPerformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m189\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrect_boolean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_boolean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-e6a0855e6dff>\u001b[0m in \u001b[0;36mgetScenarioModelPerformance\u001b[1;34m(width, num_epochs, seed_val, rect_boolean, test_boolean)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#visualizeCNN(m, s, width, images_per_class = 4, trial_seed = seed_val, testing = test_boolean)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mperf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mperf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Scenario'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mperf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2933\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2934\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2935\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2963\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2964\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2965\u001b[1;33m                 indexer = self.loc._get_listlike_indexer(\n\u001b[0m\u001b[0;32m   2966\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2967\u001b[0m                 )[1]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m         self._validate_read_indexer(\n\u001b[0m\u001b[0;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Scenario'], dtype='object')] are in the [columns]\""
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "getScenarioModelPerformance(width=189, num_epochs=13, seed_val = 2, rect_boolean = False, test_boolean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
