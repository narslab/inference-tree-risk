{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # used for loading images\n",
    "import numpy as np\n",
    "import os # used for navigating to image path\n",
    "import imageio # used for writing images\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\"\"\"HP Tuning\"\"\"\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import IPython\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import kerastuner as kt\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load numpy output files\"\"\"\n",
    "pr_im_64 = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_Im.npy', allow_pickle=True)\n",
    "pr_po_im_64 = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_Po_Im.npy', allow_pickle=True)\n",
    "pr_poim_64 = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_PoIm.npy', allow_pickle=True)\n",
    "prpo_im_64 = np.load('../../data/tidy/preprocessed_images/size64_exp5_PrPo_Im.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "NUM_CHANNEL = 1\n",
    "CLASSIFICATION_SCENARIO = \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(image_array, prop = 0.80, seed_num = 111):\n",
    "    \"\"\"Returns training and test arrays of images with specified proportion - prop:1-prop\"\"\"\n",
    "    random.Random(seed_num).shuffle(image_array)\n",
    "    train_size = int(prop*np.shape(image_array)[0])\n",
    "    train = image_array[:train_size]\n",
    "    test = image_array[train_size:]\n",
    "    return(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = splitData(pr_im_64, seed_num = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageShape(image_array):\n",
    "    if NUM_CHANNEL==1:\n",
    "        image_shape = np.array([np.expand_dims(x[0],axis=2) for x in image_array]).shape[1:4]\n",
    "    elif NUM_CHANNEL==3:\n",
    "        image_shape = np.array([x[0] for x in image_array]).shape[1:4][::-1]\n",
    "    print(image_shape)\n",
    "    return image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_shape = getImageShape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "if NUM_CHANNEL == 1:\n",
    "    train_array = np.array([np.expand_dims(x[0],axis=2) for x in train_data])\n",
    "    validation_array = np.array([np.expand_dims(x[0],axis=2) for x in test_data])\n",
    "elif NUM_CHANNEL == 3:\n",
    "    train_array = np.array([x[0] for x in train_data]) \n",
    "    train_array = np.moveaxis(train_array, 1, -1)\n",
    "    validation_array = np.array([x[0] for x in test_data])\n",
    "    validation_array = np.moveaxis(validation_array, 1, -1)\n",
    "\n",
    "train_labels = np.array([x[1] for x in train_data])\n",
    "validation_labels = np.array([x[1] for x in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = models.Sequential()\n",
    "  \n",
    "  ## Vary kernel size in first Conv Layer between 5 and 7\n",
    "  hp_k_size = hp.Choice('kernel_size', values = [5, 7])\n",
    "  \n",
    "  # Tune the number of units in the first and second Dense layers\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units_l1 = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units_l2 = hp.Int('units', min_value = 32, max_value = 512, step = 32)  \n",
    "    \n",
    "  dropout_rate_a = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "  dropout_rate_b = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "    \n",
    "  # Experiment with \"relu\" and \"tanh\" activation f-ns\n",
    "  hp_dl1_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "  hp_dl2_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    \n",
    "  model.add(layers.Conv2D(filters = 64, kernel_size = hp_k_size, strides = 2, activation=\"relu\", padding=\"same\", input_shape = input_image_shape))\n",
    "\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Flatten())\n",
    "  \n",
    "  model.add(layers.Dense(units = hp_units_l1, activation = hp_dl1_activation))\n",
    "  model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "  model.add(layers.Dropout(dropout_rate_a))\n",
    "  model.add(layers.Dense(units = hp_units_l2, activation = hp_dl2_activation))\n",
    "  model.add(layers.Dropout(dropout_rate_b))\n",
    "  model.add(keras.layers.Dense(NUM_CLASS, activation='softmax'))\n",
    "    \n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.CategoricalCrossentropy(from_logits = True), #keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "                metrics = ['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'intro_to_kt_Pr_Im')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_array, train_labels, epochs = 10, validation_data = (validation_array, validation_labels), callbacks = [ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return best hyperparameter dictionary\n",
    "best_hps_dict = tuner.get_best_hyperparameters()[0].values\n",
    "best_hps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "model_pr_im = tuner.hypermodel.build(best_hps)\n",
    "start = timer()\n",
    "\n",
    "model_pr_im.fit(train_array, train_labels, epochs=10, validation_data=(validation_array, validation_labels))\n",
    "\n",
    "end = timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pr_im.summary()\n",
    "model_path = '../../results/models/optimized_scenario_' + CLASSIFICATION_SCENARIO\n",
    "model_pr_im.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hperparamter summary\n",
    "best_hps_summary = f\"\"\"\n",
    "The hyperparameter search for classification scenario {CLASSIFICATION_SCENARIO} is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')}. The optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}. The optimal kernel size of the first convolution layer is {best_hps.get('kernel_size')}.\n",
    "The optimal dropout rate for the optimizer is {best_hps.get('dropout')}. The optimal activation layer for the optimizer is {best_hps.get('activation')}.\n",
    "\"\"\"\n",
    "\n",
    "best_hps_summary_path = model_path + '/best_hyperparameters_summary_scenario_' + CLASSIFICATION_SCENARIO + '.txt'\n",
    "print(best_hps_summary,  file=open(best_hps_summary_path, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
