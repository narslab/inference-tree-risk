{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # used for loading images\n",
    "import numpy as np\n",
    "import os # used for navigating to image path\n",
    "import imageio # used for writing images\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../python/\")\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"HP Tuning\"\"\"\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import IPython\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import kerastuner as kt\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load numpy output files\"\"\"\n",
    "images_size64_exp5_Pr_Po_Im = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_Po_Im.npy', allow_pickle=True)\n",
    "images_size64_exp5_Pr_Im = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_Im.npy', allow_pickle=True)\n",
    "images_size64_exp5_PrPo_Im = np.load('../../data/tidy/preprocessed_images/size64_exp5_PrPo_Im.npy', allow_pickle=True)\n",
    "images_size64_exp5_Pr_PoIm = np.load('../../data/tidy/preprocessed_images/size64_exp5_Pr_PoIm.npy', allow_pickle=True)\n",
    "\n",
    "images_size128_exp5_Pr_Po_Im = np.load('../../data/tidy/preprocessed_images/size128_exp5_Pr_Po_Im.npy', allow_pickle=True)\n",
    "images_size128_exp5_Pr_Im = np.load('../../data/tidy/preprocessed_images/size128_exp5_Pr_Im.npy', allow_pickle=True)\n",
    "images_size128_exp5_PrPo_Im = np.load('../../data/tidy/preprocessed_images/size128_exp5_PrPo_Im.npy', allow_pickle=True)\n",
    "images_size128_exp5_Pr_PoIm = np.load('../../data/tidy/preprocessed_images/size128_exp5_Pr_PoIm.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "NUM_CLASS_PR_PO_IM = 3\n",
    "NUM_CHANNELS = 1\n",
    "scenario_list = [\"Pr_Im\", \"PrPo_Im\", \"Pr_PoIm\", \"Pr_Po_Im\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_64 = [images_size64_exp5_Pr_Im, images_size64_exp5_PrPo_Im, images_size64_exp5_Pr_PoIm, images_size64_exp5_Pr_Po_Im]\n",
    "image_set_128 = [images_size128_exp5_Pr_Im, images_size128_exp5_PrPo_Im, images_size128_exp5_Pr_PoIm, images_size128_exp5_Pr_Po_Im]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_shape_64 = getImageShape(images_size64_exp5_Pr_Im, num_channels = NUM_CHANNELS)\n",
    "input_image_shape_128 = getImageShape(images_size128_exp5_Pr_Im, num_channels = NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder_64_2cl(hp):\n",
    "  model = models.Sequential()\n",
    "  \n",
    "  ## Vary kernel size in first Conv Layer between 5 and 7\n",
    "  hp_k_size = hp.Choice('kernel_size', values = [5, 7])\n",
    "  \n",
    "  # Tune the number of units in the first and second Dense layers\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units_l1 = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units_l2 = hp.Int('units', min_value = 32, max_value = 512, step = 32)  \n",
    "    \n",
    "  dropout_rate_a = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "  dropout_rate_b = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "    \n",
    "  # Experiment with \"relu\" and \"tanh\" activation f-ns\n",
    "  hp_dl1_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "  hp_dl2_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    \n",
    "  model.add(layers.Conv2D(filters = 64, kernel_size = hp_k_size, strides = 2, activation=\"relu\", padding=\"same\", input_shape = input_image_shape_64))\n",
    "\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Flatten())\n",
    "  \n",
    "  model.add(layers.Dense(units = hp_units_l1, activation = hp_dl1_activation))\n",
    "  model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "  model.add(layers.Dropout(dropout_rate_a))\n",
    "  model.add(layers.Dense(units = hp_units_l2, activation = hp_dl2_activation))\n",
    "  model.add(layers.Dropout(dropout_rate_b))\n",
    "  model.add(keras.layers.Dense(NUM_CLASS, activation='softmax'))\n",
    "    \n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.CategoricalCrossentropy(from_logits = True), #keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "                metrics = ['accuracy'])\n",
    "  return model\n",
    "\n",
    "def model_builder_64_3cl(hp):\n",
    "  model = models.Sequential()\n",
    "  \n",
    "  ## Vary kernel size in first Conv Layer between 5 and 7\n",
    "  hp_k_size = hp.Choice('kernel_size', values = [5, 7])\n",
    "  \n",
    "  # Tune the number of units in the first and second Dense layers\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units_l1 = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units_l2 = hp.Int('units', min_value = 32, max_value = 512, step = 32)  \n",
    "    \n",
    "  dropout_rate_a = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "  dropout_rate_b = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "    \n",
    "  # Experiment with \"relu\" and \"tanh\" activation f-ns\n",
    "  hp_dl1_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "  hp_dl2_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    \n",
    "  model.add(layers.Conv2D(filters = 64, kernel_size = hp_k_size, strides = 2, activation=\"relu\", padding=\"same\", input_shape = input_image_shape_64))\n",
    "\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Flatten())\n",
    "  \n",
    "  model.add(layers.Dense(units = hp_units_l1, activation = hp_dl1_activation))\n",
    "  model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "  model.add(layers.Dropout(dropout_rate_a))\n",
    "  model.add(layers.Dense(units = hp_units_l2, activation = hp_dl2_activation))\n",
    "  model.add(layers.Dropout(dropout_rate_b))\n",
    "  model.add(keras.layers.Dense(NUM_CLASS_PR_PO_IM, activation='softmax'))\n",
    "    \n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.CategoricalCrossentropy(from_logits = True), #keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "                metrics = ['accuracy'])\n",
    "  return model\n",
    "\n",
    "def model_builder_128_2cl(hp):\n",
    "  model = models.Sequential()\n",
    "  \n",
    "  ## Vary kernel size in first Conv Layer between 5 and 7\n",
    "  hp_k_size = hp.Choice('kernel_size', values = [5, 7])\n",
    "  \n",
    "  # Tune the number of units in the first and second Dense layers\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units_l1 = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units_l2 = hp.Int('units', min_value = 32, max_value = 512, step = 32)  \n",
    "    \n",
    "  dropout_rate_a = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "  dropout_rate_b = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "    \n",
    "  # Experiment with \"relu\" and \"tanh\" activation f-ns\n",
    "  hp_dl1_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "  hp_dl2_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    \n",
    "  model.add(layers.Conv2D(filters = 64, kernel_size = hp_k_size, strides = 2, activation=\"relu\", padding=\"same\", input_shape = input_image_shape_128))\n",
    "\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Flatten())\n",
    "  \n",
    "  model.add(layers.Dense(units = hp_units_l1, activation = hp_dl1_activation))\n",
    "  model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "  model.add(layers.Dropout(dropout_rate_a))\n",
    "  model.add(layers.Dense(units = hp_units_l2, activation = hp_dl2_activation))\n",
    "  model.add(layers.Dropout(dropout_rate_b))\n",
    "  model.add(keras.layers.Dense(NUM_CLASS, activation='softmax'))\n",
    "    \n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.CategoricalCrossentropy(from_logits = True), #keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "                metrics = ['accuracy'])\n",
    "  return model\n",
    "\n",
    "def model_builder_128_3cl(hp):\n",
    "  model = models.Sequential()\n",
    "  \n",
    "  ## Vary kernel size in first Conv Layer between 5 and 7\n",
    "  hp_k_size = hp.Choice('kernel_size', values = [5, 7])\n",
    "  \n",
    "  # Tune the number of units in the first and second Dense layers\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units_l1 = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units_l2 = hp.Int('units', min_value = 32, max_value = 512, step = 32)  \n",
    "    \n",
    "  dropout_rate_a = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "  dropout_rate_b = hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1)\n",
    "    \n",
    "  # Experiment with \"relu\" and \"tanh\" activation f-ns\n",
    "  hp_dl1_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "  hp_dl2_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    \n",
    "  model.add(layers.Conv2D(filters = 64, kernel_size = hp_k_size, strides = 2, activation=\"relu\", padding=\"same\", input_shape = input_image_shape_128))\n",
    "\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n",
    "  model.add(layers.MaxPooling2D(2))\n",
    "  model.add(layers.Flatten())\n",
    "  \n",
    "  model.add(layers.Dense(units = hp_units_l1, activation = hp_dl1_activation))\n",
    "  model.add(layers.BatchNormalization()) # Networks train faster & converge much more quickly\n",
    "  model.add(layers.Dropout(dropout_rate_a))\n",
    "  model.add(layers.Dense(units = hp_units_l2, activation = hp_dl2_activation))\n",
    "  model.add(layers.Dropout(dropout_rate_b))\n",
    "  model.add(keras.layers.Dense(NUM_CLASS_PR_PO_IM, activation='softmax'))\n",
    "    \n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.CategoricalCrossentropy(from_logits = True), #keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "                metrics = ['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_64_2cl = kt.Hyperband(model_builder_64_2cl,\n",
    "                     objective = 'loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'opt',\n",
    "                     project_name = 'imageset_64px_2_class')\n",
    "\n",
    "tuner_64_3cl = kt.Hyperband(model_builder_64_3cl,\n",
    "                     objective = 'loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'opt',\n",
    "                     project_name = 'imageset_64px_3_class')\n",
    "\n",
    "tuner_128_2cl = kt.Hyperband(model_builder_128_2cl,\n",
    "                     objective = 'loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'opt',\n",
    "                     project_name = 'imageset_128px_2_class')\n",
    "\n",
    "tuner_128_3cl = kt.Hyperband(model_builder_128_3cl,\n",
    "                     objective = 'loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'opt',\n",
    "                     project_name = 'imageset_128px_3_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hperparamter summary\n",
    "#best_hps_summary = f\"\"\"\n",
    "#The hyperparameter search for classification scenario {CLASSIFICATION_SCENARIO} is complete. The optimal number of units in the first densely-connected\n",
    "#layer is {best_hps.get('units')}. The optimal learning rate for the optimizer\n",
    "#is {best_hps.get('learning_rate')}. The optimal kernel size of the first convolution layer is {best_hps.get('kernel_size')}.\n",
    "#The optimal dropout rate for the optimizer is {best_hps.get('dropout')}. The optimal activation layer for the optimizer is {best_hps.get('activation')}.\n",
    "#\"\"\"\n",
    "#\n",
    "#best_hps_summary_path = model_path + '/best_hyperparameters_summary_scenario_' + CLASSIFICATION_SCENARIO + '.txt'\n",
    "#print(best_hps_summary,  file=open(best_hps_summary_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_64 = '../../results/models/64/'\n",
    "MODEL_PATH_128 = '../../results/models/128/'\n",
    "if not os.path.exists(MODEL_PATH_64): \n",
    "        os.makedirs(MODEL_PATH_64)\n",
    "if not os.path.exists(MODEL_PATH_128): \n",
    "        os.makedirs(MODEL_PATH_128)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_fit(tuner, model_path):\n",
    "    tuner.search(training_images, training_labels, epochs = 10, validation_data = (validation_images, validation_labels), callbacks = [ClearTrainingOutput()])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "    best_hps_dict = best_hps.values\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model.fit(training_images, training_labels, epochs=10, validation_data=(validation_images, validation_labels))\n",
    "    best_model_json = best_model.to_json()\n",
    "    scenario_path = os.path.join(model_path, scenario_list[k])\n",
    "    attr_path = os.path.join(scenario_path, 'attributes')\n",
    "    best_hps_path = 'hyperparameters_'+scenario_list[k]+'.txt'\n",
    "    model_summary_path = 'model_summary_'+scenario_list[k]+'.txt'\n",
    "    model_to_json_path = 'model_to_json_'+scenario_list[k]+'.txt'\n",
    "    if not os.path.exists(attr_path):\n",
    "        os.makedirs(attr_path)\n",
    "    with open(os.path.join(attr_path, best_hps_path), 'w') as f:\n",
    "        f.write(json.dumps(best_hps_dict))\n",
    "    with open(os.path.join(attr_path, model_summary_path), 'w') as f:\n",
    "        best_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    with open(os.path.join(attr_path, model_to_json_path), 'w') as f:\n",
    "        f.write(json.dumps(best_model_json))\n",
    "    best_model.save(scenario_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for is_64 in image_set_64:\n",
    "    training_images_and_labels, test_images_and_labels = splitData(is_64, prop = 0.80, seed_num = 1)\n",
    "    training_images, training_labels = getImageAndLabelArrays(training_images_and_labels)\n",
    "    validation_images, validation_labels = getImageAndLabelArrays(test_images_and_labels)\n",
    "    print(\"Beginning search for scenario: \" + scenario_list[k])\n",
    "    if(is_64 != images_size64_exp5_Pr_Po_Im):\n",
    "        best_model_fit(tuner_64_2cl, MODEL_PATH_64)\n",
    "    else:\n",
    "        best_model_fit(tuner_64_3cl, MODEL_PATH_64)\n",
    "    print(\"Search for scenario: \" + scenario_list[k] + \" is complete.\")\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "for is_128 in image_set_128:\n",
    "    training_images_and_labels, test_images_and_labels = splitData(is_128, prop = 0.80, seed_num = 1)\n",
    "    training_images, training_labels = getImageAndLabelArrays(training_images_and_labels)\n",
    "    validation_images, validation_labels = getImageAndLabelArrays(test_images_and_labels)\n",
    "    print(\"Beginning search for scenario: \" + scenario_list[k])\n",
    "    if(is_128 != images_size128_exp5_Pr_Po_Im):\n",
    "        best_model_fit(tuner_128_2cl, MODEL_PATH_128)\n",
    "    else:\n",
    "        best_model_fit(tuner_128_3cl, MODEL_PATH_128)\n",
    "    print(\"Search for scenario: \" + scenario_list[k] + \" is complete.\")\n",
    "    k=k+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
